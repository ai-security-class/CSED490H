<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Objectives | CSED490H</title>
    <link rel="icon" type="image/png" href="https://sangdon.github.io/media/icon_hu_cf4646339ec87b25.png" />
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=Merriweather:wght@700;900&family=Source+Sans+3:wght@400;600;700&display=swap"
      rel="stylesheet"
    />
    <link rel="stylesheet" href="../assets/css/style.css" />
  </head>
  <body>
    <header class="site-header">
      <div class="wrap header-inner">
        <a class="brand" href="/CSED490H/2026/">CSED490H · AI Security<small>POSTECH CSE / GSAI</small></a>
        <nav class="nav">
          <a href="/CSED490H/2026/">Home</a>
          <a href="/CSED490H/2026/schedule/">Schedule</a>
          <a class="active" href="/CSED490H/2026/objectives/">Objectives</a>
        </nav>
      </div>
    </header>

    <main class="wrap">
      <section class="hero" style="padding-bottom: 20px;">
        <h1>Course Objectives</h1>
      </section>

      <section class="card">
        <p>
          As AI advances and becomes practical at scale, safety and security concerns are rapidly
          emerging. In this course, we learn the art of attacking AI systems together with the
          core concepts and tools needed in modern AI.
        </p>
        <p>
          In particular, we study two core axes: <strong>victim models</strong> (e.g., LLMs, VLAs,
          and Agentic AI) and <strong>attack methods</strong> (e.g., adversarial examples and
          jailbreaking), along with optimization tools such as gradient descent, policy
          optimization, and prompt tuning with LoRA.
        </p>
        <p>
          By the end of this class, students will have a strong understanding of trendy AI model
          families, broad AI red teaming methods, and practical AI tooling required for security
          research and engineering.
        </p>
      </section>

      <section class="card">
        <h2>Learning Outcomes</h2>
        <ul>
          <li>Understand core AI model families and their security-relevant behavior.</li>
          <li>Analyze and implement inference-time and training-time attack strategies.</li>
          <li>Apply optimization techniques for whitebox and blackbox settings.</li>
          <li>Evaluate AI systems with a red teaming mindset and modern tooling.</li>
        </ul>
      </section>

      <section class="card">
        <h2>Syllabus Highlights</h2>
        <ul>
          <li><strong>Prerequisite:</strong> Artificial Intelligence</li>
          <li><strong>Course Format:</strong> Lecture + student presentations + discussions</li>
          <li><strong>Assessment:</strong> Assignment/Presentation 80%, Participation 20%</li>
          <li><strong>Reference Papers:</strong> Goodfellow et al. (ICLR 2015), Vaswani et al. (NeurIPS 2017), Schulman et al. (ICML 2015)</li>
        </ul>
        <p class="note" style="margin-bottom: 0;">
          See details in <a href="/CSED490H/2026/Syllabus.PDF" download>Syllabus.PDF</a>.
        </p>
      </section>
    </main>

    <footer class="site-footer">
      <div class="wrap">CSED490H · AI Security · POSTECH</div>
    </footer>
  </body>
</html>
<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Objectives | CSED490H</title>
    <link rel="icon" type="image/png" href="https://sangdon.github.io/media/icon_hu_cf4646339ec87b25.png" />
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=Merriweather:wght@700;900&family=Source+Sans+3:wght@400;600;700&display=swap"
      rel="stylesheet"
    />
    <link rel="stylesheet" href="../assets/css/style.css" />
  </head>
  <body>
    <header class="site-header">
      <div class="wrap header-inner">
        <a class="brand" href="/CSED490H/2026/">CSED490H · AI Security<small>POSTECH CSE / GSAI</small></a>
      </div>
    </header>

    <main class="wrap">
      <section class="hero hero-compact">
        <h1>Course Objectives</h1>
      </section>

      <section class="card">
        <p>
          As AI advances and is being practical, safety and security concerns are dramatically
          emerging. In this class, we learn the art of attacking AI systems along with necessary
          concepts and tools in AI.
        </p>
        <p>
          In particular, we will learn two core concepts, victim models (e.g., LLMs, VLAs, and
          Agentic AI) and attack methods (e.g., adversarial examples and jailbreaking) along with
          core optimization tools (e.g., gradient descent, policy optimization, and prompt tuning
          with LoRA).
        </p>
        <p>
          At the end of this class, students will have good understanding over trendy AI models,
          broad aspects of AI red teaming methods, and necessary AI tools.
        </p>
      </section>
    </main>

    <footer class="site-footer">
      <div class="wrap">CSED490H · AI Security · POSTECH</div>
    </footer>
  </body>
</html>
